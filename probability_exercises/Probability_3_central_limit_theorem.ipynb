{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["NWtcJPGlD6oi","WVeU_aSiw9MX"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# various imports you'll need later on\n","%matplotlib inline\n","import numpy as np\n","from scipy import stats\n","import random\n","import pandas as pd\n","import math\n","\n"],"metadata":{"id":"OL4RnBUMFl_e","executionInfo":{"status":"ok","timestamp":1726128268993,"user_tz":-120,"elapsed":309,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Introduction to Probability - Day 2 - Part 1\n","\n","Welcome ðŸ˜€ This is the practical session for the second part of the probability course, September 12th, 2024. The first part of the exercise teaches you how to sample from a normal distribution using the Central Limit Theorem and the second part takes you to the Newcombâ€™s speed of light story.\n","\n","\n","---\n","\n","\n","Prepared by:\\\n","Javier Gamazo Tejero\\\n","Lukas Zbinden\n","\n","---\n"],"metadata":{"id":"NWtcJPGlD6oi"}},{"cell_type":"markdown","source":["## Sampling from a Normal Distribution using the Central Limit Theorem\n","\n","### Normal Distribution\n","In this exercise we are going to be sampling from a Gaussian distribution using  the Central Limit Theorem.\n","\n","A normal distribution (also Gaussian distribution) $N \\sim (\\mu, Ïƒ)$ has a probability density function (PDF) that is parameterized by its mean ($Î¼$) and standard deviation ($Ïƒ)$ as follows:\n","\n","\\begin{align}\n","f_N(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\tag{1}\n","\\end{align}\n","\n","\n","\\\n","**Exercise 1:** Implement the PDF of a normal distribution using only numpy and calculate $f_N(x)$ for $x = \\{-1, 0, 1, 100\\}$ with Î¼ = 0 and Ïƒ = 1.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GXer05SvFnte"}},{"cell_type":"code","source":["def f_N(x, mu=0, sigma=1):\n","  # Exercise 1 TODO\n","\n","\n","print(f\"f_N(-1) = \", f_N(-1)) =\n","print(f\"f_N(0) = \", f_N(0))\n","print(f\"f_N(1) = \", f_N(1))\n","print(f\"f_N(100) = \", f_N(100))"],"metadata":{"id":"tBAtkAnCyCsg","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 2** What is the meaning of the value you get from $f_N$ for a given x?"],"metadata":{"id":"aYqgAWqTyJp0"}},{"cell_type":"code","source":["# Exercise 2 TODO"],"metadata":{"id":"9oTzQbgZysWy","executionInfo":{"status":"aborted","timestamp":1726128270587,"user_tz":-120,"elapsed":11,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\\\n","A Gaussian distribution also has a cumulative distribution function (CDF) defined as follows:\n","\n","$$ F_N(x) = \\int_{-\\infty}^{x}f_N(t) dt  \\tag{2}$$\n","\n","\\\n","**Exercise 3:** Implement the CDF of a normal distribution with Î¼ = 0 and Ïƒ = 1 using only numpy. Use the left Riemann integral (see [3], with $a = -âˆž$ and $b=x$, you can ignore the 'Midpoint Rule') to approximate the CDF. Hint: in your code, instead of $a = -âˆž$, use a large negative number.\n","\\\n","Test your implementation with $x=\\{-1, 0, 5\\}$ and validate your results with the scipy function stats.norm.cdf.\n","\n"],"metadata":{"id":"uOagDVqBy37Y"}},{"cell_type":"code","source":["\n","def F_N(x):\n","  # Exercise 3 TODO\n","  # Hint: Use the Riemann Integral approximation in\n","  # https://pythonnumericalmethods.berkeley.edu/notebooks/chapter21.02-Riemanns-Integral.html\n","\n","\n","print(f\"F_N(-1) = \", F_N(-1))\n","print(f\"F_N(0) = \", F_N(0))\n","print(f\"F_N(5) = \", F_N(5))"],"metadata":{"id":"LgrRBSM3y9vY","executionInfo":{"status":"aborted","timestamp":1726128270587,"user_tz":-120,"elapsed":11,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.linspace(a, b, n)"],"metadata":{"id":"q4S_8RW7y7T5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"CDF_N(-1) = \", stats.norm.cdf(-1, loc=0, scale=1))\n","print(f\"CDF_N(0)  = \", stats.norm.cdf(0, loc=0, scale=1))\n","print(f\"CDF_N(5)  = \", stats.norm.cdf(5, loc=0, scale=1))\n","\n"],"metadata":{"id":"2P96EgZMhViW","executionInfo":{"status":"aborted","timestamp":1726128270588,"user_tz":-120,"elapsed":10,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise 4** What is the meaning of the value you get from $F_N$ for a given x?"],"metadata":{"id":"_rgBfSgdy_Zz"}},{"cell_type":"code","source":["# Exercise 4 TODO"],"metadata":{"id":"NpSfF5BizDcO","executionInfo":{"status":"aborted","timestamp":1726128270588,"user_tz":-120,"elapsed":10,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A very common thing to do with a probability distribution is to sample from it. In other words, we want to randomly generate numbers (i.e. x values) such that the values of x are in proportion to the PDF.\n","\n","\\\n","\n","**Exercise 5** Assume you have $N \\sim (0, 1)$. How many values fall within $[-2, 2]$? How many within $[-3,3]$?\n","\\\n","Hint: two approaches you can use: 1) extend your implementation of $F_N(x)$ in exercise 3 with two parameters $a$ and $b$, and compare it with 2) use that $P(a \\leq X \\leq b) = F_N(b) - F_N(a)$ [4]."],"metadata":{"id":"-Y2qy-g1zE4J"}},{"cell_type":"code","source":["def F_N_interval(a, b):\n","    # Exercise 5 TODO\n","\n","\n","print(f\"F_N_interval(-2, 2) = \", F_N_interval(-2, 2))\n","print(f\"F_N_interval(-3, 3) = \", F_N_interval(-3, 3))"],"metadata":{"id":"sAmkP-a5zFzB","executionInfo":{"status":"aborted","timestamp":1726128270588,"user_tz":-120,"elapsed":9,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Central Limit Theorem\n","\n","The central limit theorem (CLT) is quite a surprising result relating the sample average of \\\\(n\\\\) [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d.) random variables and a normal distribution.  To state it more precisely:\n","\n","> Let \\\\({X_1, X_2, \\ldots, X_n}\\\\) be \\\\(n\\\\) i.i.d. random variables with \\\\(E(X_i)=\\mu\\\\)\n","> and \\\\(Var(X_i)=\\sigma^2\\\\) and  let \\\\(S_n = \\frac{X_1 + X_2 + \\ldots + X_n}{n}\\\\) be the sample\n","> average. Then \\\\(S_n\\\\) approximates a normal distribution with mean of \\\\(\\mu\\\\) and\n","> variance of \\\\(\\frac{\\sigma^2}{n}\\\\) for large \\\\(n\\\\) (i.e. \\\\(S_n \\approx N(\\mu, \\frac{\\sigma^2}{n})\\\\)).\n","\n","The surprising result is that \\\\(X_i\\\\) can come from *any* distribution.  It isn't restricted to just normal distributions.  We can also define the standard normal distribution in terms of \\\\(S_n\\\\) by shifting and scaling it:\n","\n","$$ N(0,1) \\approx \\frac{S_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma} \\tag{3} $$"],"metadata":{"id":"WVeU_aSiw9MX"}},{"cell_type":"markdown","source":["### Comparing Distributions\n","Since our goal is to implement sampling from a normal distribution, it would be nice to know if we actually did it correctly!  One common way to test if two arbitrary distributions are the same is to use the [Kolmogorovâ€“Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).  In the basic form, we can compare a sample of points with a reference distribution to find their similarity.  \n","\n","The basic idea of the test is to first sort the points in the sample and then compute the [empirical CDF](https://en.wikipedia.org/wiki/Empirical_distribution_function). Next, find the largest absolute difference between any point in the empirical CDF and the theoretical reference distribution.  If the two are the same, this difference should be very small. If it's large then we can be confident that the distribution is different.  Further, this difference follows a certain distribution, which allows us to test our null hypothesis of whether our samples were drawn from the reference distribution.  The following figure shows this more clearly:\n","![Figure](https://upload.wikimedia.org/wikipedia/commons/3/3f/KS2_Example.png)\n","Figure: Illustration of the two-sample Kolmogorovâ€“Smirnov statistic. Red and blue lines each correspond to an empirical distribution function, and the black arrow is the two-sample KS statistic.\n","\n","Fortunately, we don't have to implement this ourselves.  A package is available in [scipy.stats](http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html).  Let's play around with it a bit to see how it works."],"metadata":{"id":"TPCX6lRW0oCU"}},{"cell_type":"code","source":["np.random.seed(123)\n","\n","# number of samples\n","N=10000\n","\n","# Run Kolmogorov-Smirnov test Uniform(0,1) vs. reference N(0, 1)\n","samples = stats.uniform.rvs(loc=0, scale=1, size=N)\n","test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n","print(\"U(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n","\n","# Run Kolmogorov-Smirnov test N(0, 2) vs. reference N(0, 1)\n","samples = stats.norm.rvs(loc=0, scale=2, size=N)\n","test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n","print(\"N(0,2) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n","\n","# Run Kolmogorov-Smirnov test N(0, 1) vs. reference N(0, 1)\n","samples = stats.norm.rvs(loc=0, scale=1, size=N)\n","test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n","print(\"N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))"],"metadata":{"id":"sQPxxRXC2Z22","executionInfo":{"status":"aborted","timestamp":1726128270588,"user_tz":-120,"elapsed":9,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using \\\\(N(0,1)\\\\) as our reference distribution, the KS test has a large value and a negligible p-value when comparing to a uniform distribution \\\\(U(0,1)\\\\) (\\\\(KS=0.5\\\\)).  The normal distribution with a wider base \\\\(N(0, 2)\\\\) (\\\\(KS=0.1673\\\\)) also has a negligible p-value.  However, when we compare samples from the identical distribution \\\\(N(0,1)\\\\), we get a relatively small value (\\\\(KS=0.0104\\\\)) for the test statistic and a large p-value, indicating we don't have sufficient evidence to reject the null hypothesis (that our two distributions are the same)."],"metadata":{"id":"IFacRQQP2lfA"}},{"cell_type":"markdown","source":["### Sampling using the Central Limit Theorem\n","Recall the Central Limit Theorem from the paragraph above. Our aim now is to sample from \\\\(N(0,1)\\\\) using the Central Limit Theorem (CLT).\n","\n","\\\n","In the subsequent example, using the CLT, we can transform a given i.i.d variable $X_n$ that follows a Bernoulli distribution in such a way that we achieve an approximate standard normal distribution, which we can then use to draw samples from. A practical application of that is the need to generate pseudo-random numbers following a standard normal distribution (other distributions as well)."],"metadata":{"id":"0_BKoiFx2qt0"}},{"cell_type":"markdown","source":["First let's define our i.i.d. variable \\\\(X_n\\\\) to have a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with \\\\(p=0.5\\\\) which we can intuitively think of tossing an unbiased coin (note well: \\\\(X_n\\\\) could have any distribution here):\n","\n","$$ P(X_n=k) = \\begin{cases} p=0.5 & \\text{if }k=1, \\\\[6pt] 1-p = 0.5 & \\text {if }k=0.\\end{cases} \\tag{4} $$\n","\n","Recall, the Bernoulli distribution closely relates to the [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) denoted by \\\\(B(n, p)\\\\) by \\\\(Bernoulli(p) = B(n=1, p)\\\\) .  The Binomial distribution can intuitively be thought of as counting the number of heads in \\\\(n\\\\) tosses of a coin (i.e. Bernoulli trials).  If \\\\(n=1\\\\), it reduces to a Bernoulli distribution (or single coin toss).\n","\n","Let's now define our sample average for \\\\(n\\\\) tosses of our unbiased coin (cf. definition of CLT):\n","\n","$$ S_n = \\frac{X_1 + X_2 + \\ldots + X_n}{n} = \\frac{B(n, p=0.5)}{n} $$\n","\n","We know that a Bernoulli distribution has \\\\(\\mu=\\frac{1}{2}\\\\) (we expect half our tosses to be heads), and \\\\(\\sigma^2=p(1-p)=0.25\\\\).\n","\n","Shifting and scaling this to get our standard normal distribution using Equation 3, we get:\n","\n","\\begin{align}\n","N(0,1) &\\approx \\frac{\\sqrt{n}(S_n - \\mu)}{\\sigma} \\\\\n","       &= \\frac{\\sqrt{n}(\\frac{X_1 + X_2 + \\ldots + X_n}{n} - 0.5)}{\\sqrt{0.25}} \\\\\n","       &= 2\\sqrt{n}(\\frac{X_1 + X_2 + \\ldots + X_n}{n} - 0.5) \\tag{5}\n","\\end{align}\n","\n","Theoretically, this should give us an equation to roughly simulate a standard normal distribution.  Let's try to implement it!"],"metadata":{"id":"-2dhMu1cH6Rc"}},{"cell_type":"markdown","source":["**Exercise 6** Implement the above example in the following code section. Look at the Kolmogorov-Smirnov test result as well as the plot to verify if by using a Bernoulli distribution one can sample from a normal distribution thanks to the CLT."],"metadata":{"id":"_sWjQynNJM0t"}},{"cell_type":"code","source":["N = 10000\n","random.seed(123)\n","\n","def bernoulli():\n","    # TODO implement a Bernoulli distribution with p=0.5 (equation (4))\n","    return None\n","\n","def sample_normal(n=2500):\n","    # TODO implement the sampling function using equation (5), n is given as parameter\n","    return None\n","\n","\n","samples = None  # TODO create samples\n","\n","# Use Kolmogorov-Smirnov to test again if our approximation is close enough to normal\n","test_stat, pvalue = stats.kstest(samples, 'norm', args=(0, 1), N=N)\n","print(\"sample_N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.\" % (test_stat, pvalue))\n","\n","# Let's plot our samples against our reference distribution\n","reference = [stats.norm.rvs() for x in range(N)]\n","pd.DataFrame({'sample_N': samples, 'N(0,1)': reference}).plot(kind='hist', bins=100, alpha=0.4)"],"metadata":{"id":"zFPF_nbSKgfS","executionInfo":{"status":"aborted","timestamp":1726128270588,"user_tz":-120,"elapsed":9,"user":{"displayName":"Javier Gamazo Tejero","userId":"04168543225743776047"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### References and Further Reading:\n","[1] https://bjlkeng.github.io/posts/sampling-from-a-normal-distribution/\n","\n","[2] [Python Programming And Numerical Methods: A Guide For Engineers And Scientists](https://pythonnumericalmethods.berkeley.edu/notebooks/Index.html)\n","\n","[3] [Riemanns Integral](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter21.02-Riemanns-Integral.html)\n","\n","[4] [Normal distribution: (Cumulative) Distribution Function](http://www.matematicasvisuales.com/english/html/probability/varaleat/normaldistribution.html)"],"metadata":{"id":"H9EkJ2JKrNoo"}}]}